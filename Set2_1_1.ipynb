{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import numpy as np\n",
    "import os\n",
    "from logging import error, warning, info, basicConfig\n",
    "from tqdm import tqdm\n",
    "from numba import njit, jit, prange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "basicConfig(level='INFO')\n",
    "SAVE_PATH = 'temp_media/ex2'\n",
    "ANIM_BACKEND = 'ffmpeg'\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for CUDA availability for GPU compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:CUDA is not available. Uses CPU for computations instead.\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "\n",
    "CUDA = False\n",
    "\n",
    "try:\n",
    "    CUDA = cuda.detect()\n",
    "    info('CUDA is available.')\n",
    "except ImportError:\n",
    "    info('CUDA is not available. Uses CPU for computations instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOR solver for solving Laplace equation and getting concentration gradients at final iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, parallel=True)\n",
    "def sor_parallel(c, omega, epsilon=1e-6, max_iterations=10_000):\n",
    "    '''Solves the Laplace equation using the red-black parallel Successive Over-Relaxation (SOR) method introduced in Set 1'''\n",
    "    converged = False\n",
    "    Nx = c.shape[0] - 2  # Number of interior points in one dimension\n",
    "\n",
    "    for n in range(max_iterations):\n",
    "        c_old = c.copy()  # Copy the grid for convergence check\n",
    "\n",
    "        # Update red points\n",
    "        for i in prange(1, Nx+1):\n",
    "            for j in range(1, Nx+1):\n",
    "                if (i + j) % 2 == 0:\n",
    "                    c[i, j] = (1-omega)*c[i, j] + omega*0.25 * \\\n",
    "                        (c[i+1, j] + c[i-1, j] + c[i, j+1] + c[i, j-1])\n",
    "\n",
    "        # Update black points\n",
    "        for i in prange(1, Nx+1):\n",
    "            for j in range(1, Nx+1):\n",
    "                if (i + j) % 2 == 1:\n",
    "                    c[i, j] = (1-omega)*c[i, j] + omega*0.25 * \\\n",
    "                        (c[i+1, j] + c[i-1, j] + c[i, j+1] + c[i, j-1])\n",
    "\n",
    "        # Apply boundary conditions # TODO: check if these are valid\n",
    "        c[0, :] = c[-2, :]\n",
    "        c[-1, :] = c[1, :]\n",
    "\n",
    "        converged = has_converged(c, c_old, epsilon)\n",
    "        if converged:\n",
    "            info(f'Converged after {n} iterations using omega={omega}.')\n",
    "            break\n",
    "\n",
    "    return c, n, converged\n",
    "\n",
    "\n",
    "def has_converged(c_new, c_old, epsilon):\n",
    "    '''Check if the SOR solver has converged to a stead state solution.'''\n",
    "    return np.max(np.abs(c_new - c_old)) < epsilon\n",
    "\n",
    "\n",
    "def select_best_omega_for_sor(omega_values, epsilon, c_init, max_iterations=2_000):\n",
    "    '''Finds the best relaxation parameter omega for SOR'''\n",
    "\n",
    "    best_omega = 0\n",
    "    least_iter = max_iterations\n",
    "\n",
    "    for omega in tqdm(range(omega_values)):\n",
    "        c = c_init.copy()\n",
    "        c, n_iter, converged = sor_parallel(c, omega, epsilon, max_iterations)\n",
    "        if converged and n_iter < least_iter:\n",
    "            least_iter = n_iter\n",
    "            best_omega = omega\n",
    "            c_converged = c.copy()\n",
    "        else:\n",
    "            info(f'Convergence not reached for omega={omega}.')\n",
    "    if best_omega == 0:\n",
    "        raise ValueError(f'Unable to converge using provided omega values {omega_values}. Please provide valid omega range and maximum iteration count (={max_iterations}) for convergence.')\n",
    "    return c_converged, best_omega, least_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scientific",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
